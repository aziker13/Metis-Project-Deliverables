{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python [conda env:metis] *","language":"python","name":"conda-env-metis-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"leaves_NN-Copy1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"101a0f6d","outputId":"dc45075d-1b80-462c-f3a7-fbfafdb37bf1"},"source":["pip install opencv-python"],"id":"101a0f6d","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting opencv-python\n","  Downloading opencv_python-4.5.4.60-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n","\u001b[K     |████████████████████████████████| 60.3 MB 9.4 MB/s eta 0:00:011     |████████████████████████████████| 60.1 MB 9.4 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in ./anaconda3/envs/metis/lib/python3.8/site-packages (from opencv-python) (1.19.5)\n","Installing collected packages: opencv-python\n","Successfully installed opencv-python-4.5.4.60\n","Note: you may need to restart the kernel to use updated packages.\n"]}]},{"cell_type":"code","metadata":{"id":"9622d67a","executionInfo":{"status":"ok","timestamp":1638473044178,"user_tz":480,"elapsed":3041,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["import os \n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","%matplotlib inline"],"id":"9622d67a","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIJKTutfk949","executionInfo":{"status":"ok","timestamp":1638473070456,"user_tz":480,"elapsed":23894,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"398e1783-78ff-4c35-99d9-e82ecae65b18"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"mIJKTutfk949","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"80446305","executionInfo":{"status":"ok","timestamp":1638473075256,"user_tz":480,"elapsed":144,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["path = \"/content/drive/My Drive/Leaf_outlines/\""],"id":"80446305","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"5c80e1ba","executionInfo":{"status":"ok","timestamp":1638473814031,"user_tz":480,"elapsed":737238,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["training_data = []\n","ground_truth = []\n","for img in os.listdir(path):\n","    pic = cv2.imread(os.path.join(path,img))\n","    pic = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n","    pic = cv2.resize(pic,(256,256))\n","    training_data.append([pic])\n","    ground_truth.append(img[0:4])"],"id":"5c80e1ba","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGBKdZzwZqZx"},"source":["saved = np.load(os.path.join(path, 'leaf_images.npy'))"],"id":"GGBKdZzwZqZx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3d7ee1c","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1638474002157,"user_tz":480,"elapsed":141,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"0e0d8c22-489b-448a-e514-22711c06764a"},"source":["ground_truth[0]"],"id":"d3d7ee1c","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Pnit'"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"b7dc164d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638474005287,"user_tz":480,"elapsed":123,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"3e45dae1-8c10-4b23-a6d0-1727da534da0"},"source":["#Need to make integers for to_categorical later, ignoring pfii because it will merge with Pfoe later\n","d = {}\n","counter = 0\n","for i in ground_truth:\n","    if i[:4] == 'Pfii':\n","        continue\n","    elif i not in d.keys():\n","        d[i] = counter\n","        counter += 1\n","    else:\n","        continue\n","d"],"id":"b7dc164d","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Pact': 37,\n"," 'Pala': 38,\n"," 'Pame': 39,\n"," 'Pbif': 24,\n"," 'Pcae': 25,\n"," 'Pcap': 26,\n"," 'Pcin': 27,\n"," 'Pcoc': 28,\n"," 'Pcor': 29,\n"," 'Pcri': 30,\n"," 'Pedm': 31,\n"," 'Pedu': 32,\n"," 'Pfoe': 33,\n"," 'Pgal': 34,\n"," 'Pgib': 35,\n"," 'Pgra': 36,\n"," 'Phat': 12,\n"," 'Pker': 13,\n"," 'Plig': 14,\n"," 'Pmaf': 15,\n"," 'Pmal': 16,\n"," 'Pmic': 17,\n"," 'Pmie': 18,\n"," 'Pmin': 19,\n"," 'Pmis': 20,\n"," 'Pmol': 21,\n"," 'Pmor': 22,\n"," 'Pmuc': 23,\n"," 'Pnit': 0,\n"," 'Porg': 1,\n"," 'Ppoh': 2,\n"," 'Prac': 3,\n"," 'Prub': 4,\n"," 'Pset': 5,\n"," 'Psid': 6,\n"," 'Psub': 7,\n"," 'Pten': 8,\n"," 'Ptlo': 9,\n"," 'Ptri': 10,\n"," 'Pvil': 11}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"47872e4f"},"source":["np.save(os.path.join(path,'leaf_images'),np.array(training_data))"],"id":"47872e4f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d20052c5"},"source":["np.save(os.path.join(path,'leaf_truths'),np.array(ground_truth))"],"id":"d20052c5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bd277ff","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1638474021138,"user_tz":480,"elapsed":463,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"bc110935-7e3d-4a71-db45-6cb333534098"},"source":["plt.imshow(np.array(training_data[0]).reshape(256,256,3))"],"id":"2bd277ff","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f53011204d0>"]},"metadata":{},"execution_count":7},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"16b1149f","executionInfo":{"status":"ok","timestamp":1638474024176,"user_tz":480,"elapsed":124,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["import pandas as pd"],"id":"16b1149f","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"7516c43a","executionInfo":{"status":"ok","timestamp":1638474025900,"user_tz":480,"elapsed":107,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["data = {'image': training_data, 'species': ground_truth}\n","df = pd.DataFrame(data=data)"],"id":"7516c43a","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"3654e0d8","executionInfo":{"status":"ok","timestamp":1638474027571,"user_tz":480,"elapsed":136,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["'''in read.me of original data, Pfii and Pfoe both represent the same\n","    species, relabelling to reflect this'''                   \n","df.species.replace({'Pfii': 'Pfoe'}, inplace=True)                          "],"id":"3654e0d8","execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"4204f82e","executionInfo":{"status":"ok","timestamp":1638474029072,"user_tz":480,"elapsed":137,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["#rep#54lacing species name with integer value for later conversion to categoricals\n","df.species.replace(d, inplace=True)    "],"id":"4204f82e","execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"7f8e063a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638474030105,"user_tz":480,"elapsed":116,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"5e3885c9-81ea-47c0-f98c-88a1ae645159"},"source":["df.species.max()"],"id":"7f8e063a","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"66ce1b5d","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1638474045755,"user_tz":480,"elapsed":317,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"17fe3a2d-c0e4-4d1f-d725-84aeb68f6782"},"source":["plt.imshow(np.array(df.image[0]).reshape(256,256,3))"],"id":"66ce1b5d","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f5300b718d0>"]},"metadata":{},"execution_count":14},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"39a03433","executionInfo":{"status":"ok","timestamp":1638474053824,"user_tz":480,"elapsed":816,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","X_vals, y_vals = np.array(training_data), df.species.astype(np.int64)\n","\n","# 28x28 images, with grey scale pixel strengths repeated across 3 channels for color representation\n","X_vals = X_vals.reshape((-1,256,256,3)) \n","### REPEAT VALUES ACROSS 2 ADDITIONAL CHANNELS\n","\n","X_train, X_test, y_train, y_test = (train_test_split(X_vals, y_vals, \n","                                                     test_size = .2, random_state = 42))\n","\n","# 2D (one-hot encoded) representation of multiclass target \n","y_train_cat = to_categorical(y_train)"],"id":"39a03433","execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"e40194d1","outputId":"65a02cbb-f824-4275-c1ff-81fa96a47aa7"},"source":["y_train_cat.shape"],"id":"e40194d1","execution_count":null,"outputs":[{"data":{"text/plain":["(2635, 40)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"396d9513","outputId":"f0be5fa2-6cdb-433e-c8e3-31254d2a82fe"},"source":["y_vals[345]"],"id":"396d9513","execution_count":null,"outputs":[{"data":{"text/plain":["40"]},"execution_count":122,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"17db6922","outputId":"84b1f0cb-768b-4148-d313-4a36ace28778"},"source":["a = pd.get_dummies(df.species)\n","a.shape"],"id":"17db6922","execution_count":null,"outputs":[{"data":{"text/plain":["(3294, 40)"]},"execution_count":117,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"88ae4b34","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638474877273,"user_tz":480,"elapsed":806428,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"251bb959-c0ee-46d0-ba54-5a2abac27f5e"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, InputLayer\n","'''\n"," In this network structure, note that we follow the typical CNN heuristic of \n"," gradually reducing width and height dimenions over time with max pooling\n"," (typically by a factor of 2), but increasing the filter depth dimension \n"," to find increasingly specific patterns. These models are typically compromised \n"," of a series of convolutional blocks followed by a flattening operation and \n"," a series of fully connected layers at the terminus.\n","'''\n","\n","NN = Sequential()\n","\n","NN.add(InputLayer(input_shape=X_train.shape[1:]))\n","\n","# Conv block 1.  You can add more conv steps to\n","# each block to increase model capacity.\n","NN.add(Conv2D(filters=30, kernel_size=3, activation='relu', padding='same'))\n","# NN.add(Conv2D(filters=16, kernel_size=3, activation='relu', padding='same'))\n","NN.add(MaxPooling2D())\n","\n","# Conv block 2 - note we increase filter dimension as we move\n","# further into the network. You can add more conv steps to\n","# each block to increase model capacity.\n","NN.add(Conv2D(filters=60, kernel_size=3, activation='relu', padding='same'))\n","# NN.add(Conv2D(filters=16, kernel_size=3, activation='relu', padding='same'))\n","NN.add(MaxPooling2D())\n","\n","# Conv block 3 - The conv blocks should be ended with either a flatten\n","# layer or a global pooling layer. These transform the 2D layers to 1D\n","# to match the following dense layers.\n","NN.add(Conv2D(filters=90, kernel_size=3, activation='relu', padding='same'))\n","\n","NN.add(GlobalAveragePooling2D())\n","\n","# Fully connected block - flattening followed by dense and output layers\n","NN.add(Flatten())\n","NN.add(Dense(80, activation='relu'))\n","NN.add(Dense(40, activation='softmax'))  # 40 target classes\n","\n","NN.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy'],\n",")\n","NN.summary()\n","NN.fit(X_train, y_train_cat, epochs=100, verbose=1, validation_split=0.25,\n","       callbacks=[\n","           keras.callbacks.ModelCheckpoint(\n","               'models/mnist.{epoch:02d}-{val_loss:.2f}.hdf5',\n","               save_best_only=True)\n","       ])  # track progress as we fit"],"id":"88ae4b34","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 256, 256, 30)      840       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 128, 128, 30)     0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 128, 128, 60)      16260     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 64, 64, 60)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 64, 64, 90)        48690     \n","                                                                 \n"," global_average_pooling2d (G  (None, 90)               0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," flatten (Flatten)           (None, 90)                0         \n","                                                                 \n"," dense (Dense)               (None, 80)                7280      \n","                                                                 \n"," dense_1 (Dense)             (None, 40)                3240      \n","                                                                 \n","=================================================================\n","Total params: 76,310\n","Trainable params: 76,310\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","62/62 [==============================] - 23s 153ms/step - loss: 7.7807 - accuracy: 0.0299 - val_loss: 3.6141 - val_accuracy: 0.0607\n","Epoch 2/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.5714 - accuracy: 0.0592 - val_loss: 3.5169 - val_accuracy: 0.0683\n","Epoch 3/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.3105 - accuracy: 0.1048 - val_loss: 3.1028 - val_accuracy: 0.1593\n","Epoch 4/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.0169 - accuracy: 0.1452 - val_loss: 2.8260 - val_accuracy: 0.2033\n","Epoch 5/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.8061 - accuracy: 0.1888 - val_loss: 2.6794 - val_accuracy: 0.2307\n","Epoch 6/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.6991 - accuracy: 0.2171 - val_loss: 2.7478 - val_accuracy: 0.1958\n","Epoch 7/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.5986 - accuracy: 0.2201 - val_loss: 2.5116 - val_accuracy: 0.2625\n","Epoch 8/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.4731 - accuracy: 0.2647 - val_loss: 2.3484 - val_accuracy: 0.3035\n","Epoch 9/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.3795 - accuracy: 0.2854 - val_loss: 2.2519 - val_accuracy: 0.3445\n","Epoch 10/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.2232 - accuracy: 0.3381 - val_loss: 2.2096 - val_accuracy: 0.3308\n","Epoch 11/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.1170 - accuracy: 0.3684 - val_loss: 2.1016 - val_accuracy: 0.3657\n","Epoch 12/100\n","62/62 [==============================] - 8s 131ms/step - loss: 1.9887 - accuracy: 0.3932 - val_loss: 2.0863 - val_accuracy: 0.3900\n","Epoch 13/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.9308 - accuracy: 0.4150 - val_loss: 2.0267 - val_accuracy: 0.3794\n","Epoch 14/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.8412 - accuracy: 0.4312 - val_loss: 1.8911 - val_accuracy: 0.4537\n","Epoch 15/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.7373 - accuracy: 0.4590 - val_loss: 1.8585 - val_accuracy: 0.4704\n","Epoch 16/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.6721 - accuracy: 0.4823 - val_loss: 1.8528 - val_accuracy: 0.4613\n","Epoch 17/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.6522 - accuracy: 0.4889 - val_loss: 1.9773 - val_accuracy: 0.4507\n","Epoch 18/100\n","62/62 [==============================] - 8s 121ms/step - loss: 1.5866 - accuracy: 0.5187 - val_loss: 1.8782 - val_accuracy: 0.4643\n","Epoch 19/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.5111 - accuracy: 0.5380 - val_loss: 1.6760 - val_accuracy: 0.5432\n","Epoch 20/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.4754 - accuracy: 0.5329 - val_loss: 1.7172 - val_accuracy: 0.5008\n","Epoch 21/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.4987 - accuracy: 0.5324 - val_loss: 1.6666 - val_accuracy: 0.5372\n","Epoch 22/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.3773 - accuracy: 0.5956 - val_loss: 1.6464 - val_accuracy: 0.5508\n","Epoch 23/100\n","62/62 [==============================] - 8s 121ms/step - loss: 1.3252 - accuracy: 0.5860 - val_loss: 1.6696 - val_accuracy: 0.5038\n","Epoch 24/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.3337 - accuracy: 0.5719 - val_loss: 1.6274 - val_accuracy: 0.5463\n","Epoch 25/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.2755 - accuracy: 0.5987 - val_loss: 1.5464 - val_accuracy: 0.5690\n","Epoch 26/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.2487 - accuracy: 0.6058 - val_loss: 1.6317 - val_accuracy: 0.5357\n","Epoch 27/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.2087 - accuracy: 0.6179 - val_loss: 1.5483 - val_accuracy: 0.5751\n","Epoch 28/100\n","62/62 [==============================] - 7s 121ms/step - loss: 1.2040 - accuracy: 0.6301 - val_loss: 1.5707 - val_accuracy: 0.5751\n","Epoch 29/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.1655 - accuracy: 0.6316 - val_loss: 1.4917 - val_accuracy: 0.5812\n","Epoch 30/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.2090 - accuracy: 0.6098 - val_loss: 1.5506 - val_accuracy: 0.5539\n","Epoch 31/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.1199 - accuracy: 0.6382 - val_loss: 1.4828 - val_accuracy: 0.5766\n","Epoch 32/100\n","62/62 [==============================] - 8s 121ms/step - loss: 1.0842 - accuracy: 0.6544 - val_loss: 1.5887 - val_accuracy: 0.5554\n","Epoch 33/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0927 - accuracy: 0.6387 - val_loss: 1.4811 - val_accuracy: 0.5964\n","Epoch 34/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0936 - accuracy: 0.6579 - val_loss: 1.4797 - val_accuracy: 0.5979\n","Epoch 35/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0502 - accuracy: 0.6579 - val_loss: 1.6389 - val_accuracy: 0.5432\n","Epoch 36/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0281 - accuracy: 0.6711 - val_loss: 1.5306 - val_accuracy: 0.6070\n","Epoch 37/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0273 - accuracy: 0.6705 - val_loss: 1.4701 - val_accuracy: 0.5766\n","Epoch 38/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.9746 - accuracy: 0.6842 - val_loss: 1.4791 - val_accuracy: 0.6039\n","Epoch 39/100\n","62/62 [==============================] - 8s 131ms/step - loss: 1.0008 - accuracy: 0.6705 - val_loss: 1.5542 - val_accuracy: 0.5888\n","Epoch 40/100\n","62/62 [==============================] - 8s 132ms/step - loss: 0.9926 - accuracy: 0.6721 - val_loss: 1.4327 - val_accuracy: 0.6009\n","Epoch 41/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.9483 - accuracy: 0.6903 - val_loss: 1.5677 - val_accuracy: 0.5766\n","Epoch 42/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.9383 - accuracy: 0.6883 - val_loss: 1.4671 - val_accuracy: 0.6161\n","Epoch 43/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.9311 - accuracy: 0.6908 - val_loss: 1.4880 - val_accuracy: 0.6009\n","Epoch 44/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8963 - accuracy: 0.7004 - val_loss: 1.4066 - val_accuracy: 0.6419\n","Epoch 45/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8601 - accuracy: 0.7206 - val_loss: 1.4540 - val_accuracy: 0.6100\n","Epoch 46/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8361 - accuracy: 0.7277 - val_loss: 1.5431 - val_accuracy: 0.5933\n","Epoch 47/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.8482 - accuracy: 0.7050 - val_loss: 1.5082 - val_accuracy: 0.6085\n","Epoch 48/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8327 - accuracy: 0.7186 - val_loss: 1.4971 - val_accuracy: 0.6404\n","Epoch 49/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8044 - accuracy: 0.7272 - val_loss: 1.5959 - val_accuracy: 0.6024\n","Epoch 50/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8076 - accuracy: 0.7318 - val_loss: 1.5447 - val_accuracy: 0.6085\n","Epoch 51/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.8100 - accuracy: 0.7212 - val_loss: 1.4601 - val_accuracy: 0.6358\n","Epoch 52/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7592 - accuracy: 0.7399 - val_loss: 1.4989 - val_accuracy: 0.6146\n","Epoch 53/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7550 - accuracy: 0.7434 - val_loss: 1.4989 - val_accuracy: 0.6115\n","Epoch 54/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.8025 - accuracy: 0.7318 - val_loss: 1.5278 - val_accuracy: 0.6328\n","Epoch 55/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7401 - accuracy: 0.7449 - val_loss: 1.4952 - val_accuracy: 0.6282\n","Epoch 56/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7360 - accuracy: 0.7525 - val_loss: 1.6561 - val_accuracy: 0.5994\n","Epoch 57/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7843 - accuracy: 0.7277 - val_loss: 1.5354 - val_accuracy: 0.6328\n","Epoch 58/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.6798 - accuracy: 0.7733 - val_loss: 1.5088 - val_accuracy: 0.6085\n","Epoch 59/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.6877 - accuracy: 0.7718 - val_loss: 1.6161 - val_accuracy: 0.6024\n","Epoch 60/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.7499 - accuracy: 0.7460 - val_loss: 1.4593 - val_accuracy: 0.6525\n","Epoch 61/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7062 - accuracy: 0.7601 - val_loss: 1.4971 - val_accuracy: 0.6252\n","Epoch 62/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.6646 - accuracy: 0.7773 - val_loss: 1.6203 - val_accuracy: 0.6191\n","Epoch 63/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.6403 - accuracy: 0.7849 - val_loss: 1.5113 - val_accuracy: 0.6388\n","Epoch 64/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.6347 - accuracy: 0.7859 - val_loss: 1.6703 - val_accuracy: 0.6115\n","Epoch 65/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7067 - accuracy: 0.7677 - val_loss: 1.6667 - val_accuracy: 0.6176\n","Epoch 66/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.6410 - accuracy: 0.7773 - val_loss: 1.5383 - val_accuracy: 0.6601\n","Epoch 67/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.6304 - accuracy: 0.7844 - val_loss: 1.5896 - val_accuracy: 0.6115\n","Epoch 68/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.6702 - accuracy: 0.7692 - val_loss: 1.7005 - val_accuracy: 0.6070\n","Epoch 69/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.6561 - accuracy: 0.7844 - val_loss: 1.5289 - val_accuracy: 0.6464\n","Epoch 70/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.6237 - accuracy: 0.7971 - val_loss: 1.5525 - val_accuracy: 0.6616\n","Epoch 71/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.5603 - accuracy: 0.8102 - val_loss: 1.5916 - val_accuracy: 0.6313\n","Epoch 72/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.5707 - accuracy: 0.8067 - val_loss: 1.5980 - val_accuracy: 0.6480\n","Epoch 73/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.5445 - accuracy: 0.8158 - val_loss: 1.6185 - val_accuracy: 0.6358\n","Epoch 74/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.5920 - accuracy: 0.8001 - val_loss: 1.5738 - val_accuracy: 0.6510\n","Epoch 75/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.5799 - accuracy: 0.8102 - val_loss: 1.6985 - val_accuracy: 0.6464\n","Epoch 76/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.5680 - accuracy: 0.8036 - val_loss: 1.5770 - val_accuracy: 0.6586\n","Epoch 77/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.5597 - accuracy: 0.8128 - val_loss: 1.7365 - val_accuracy: 0.6131\n","Epoch 78/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.5472 - accuracy: 0.8087 - val_loss: 1.6552 - val_accuracy: 0.6464\n","Epoch 79/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.5041 - accuracy: 0.8300 - val_loss: 1.6622 - val_accuracy: 0.6571\n","Epoch 80/100\n","62/62 [==============================] - 8s 131ms/step - loss: 0.5219 - accuracy: 0.8254 - val_loss: 1.5313 - val_accuracy: 0.6783\n","Epoch 81/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.5146 - accuracy: 0.8350 - val_loss: 1.6378 - val_accuracy: 0.6419\n","Epoch 82/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.4929 - accuracy: 0.8305 - val_loss: 1.6968 - val_accuracy: 0.6434\n","Epoch 83/100\n","62/62 [==============================] - 8s 131ms/step - loss: 0.4961 - accuracy: 0.8198 - val_loss: 1.5978 - val_accuracy: 0.6707\n","Epoch 84/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.4371 - accuracy: 0.8573 - val_loss: 1.7643 - val_accuracy: 0.6631\n","Epoch 85/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.5466 - accuracy: 0.8052 - val_loss: 1.8241 - val_accuracy: 0.5994\n","Epoch 86/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.4910 - accuracy: 0.8315 - val_loss: 1.6992 - val_accuracy: 0.6510\n","Epoch 87/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.4436 - accuracy: 0.8492 - val_loss: 1.6171 - val_accuracy: 0.6495\n","Epoch 88/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.4308 - accuracy: 0.8629 - val_loss: 1.6702 - val_accuracy: 0.6813\n","Epoch 89/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.4973 - accuracy: 0.8396 - val_loss: 1.7457 - val_accuracy: 0.6525\n","Epoch 90/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.4891 - accuracy: 0.8436 - val_loss: 1.8515 - val_accuracy: 0.6297\n","Epoch 91/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.4948 - accuracy: 0.8279 - val_loss: 1.7281 - val_accuracy: 0.6662\n","Epoch 92/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.3859 - accuracy: 0.8704 - val_loss: 1.6738 - val_accuracy: 0.6677\n","Epoch 93/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.4548 - accuracy: 0.8553 - val_loss: 1.6940 - val_accuracy: 0.6464\n","Epoch 94/100\n","62/62 [==============================] - 7s 121ms/step - loss: 0.4316 - accuracy: 0.8588 - val_loss: 1.8544 - val_accuracy: 0.6571\n","Epoch 95/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.3899 - accuracy: 0.8664 - val_loss: 1.7127 - val_accuracy: 0.6737\n","Epoch 96/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.3760 - accuracy: 0.8740 - val_loss: 1.7431 - val_accuracy: 0.6737\n","Epoch 97/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.3646 - accuracy: 0.8801 - val_loss: 1.9518 - val_accuracy: 0.6480\n","Epoch 98/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.3603 - accuracy: 0.8684 - val_loss: 1.7518 - val_accuracy: 0.6646\n","Epoch 99/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.3807 - accuracy: 0.8740 - val_loss: 2.0279 - val_accuracy: 0.6404\n","Epoch 100/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.6100 - accuracy: 0.8077 - val_loss: 1.7406 - val_accuracy: 0.6601\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f52f0210350>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"3897c435","outputId":"fbcfd80c-015d-4275-9169-53aaff0ef3dd"},"source":["from sklearn.metrics import accuracy_score\n","\n","preds = np.argmax(NN.predict(X_test), axis=1)\n","accuracy_score(y_test, preds)\n"],"id":"3897c435","execution_count":null,"outputs":[{"data":{"text/plain":["0.6312594840667678"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"24ccb309"},"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","lr_confusion = confusion_matrix(y_test, preds)\n","lr_confusion\n","\n","sns.set(rc={'figure.figsize':(10,7)})\n","sns.heatmap(lr_confusion, cmap=plt.cm.Blues,fmt='d', square=True,);\n","\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('confusion matrix')"],"id":"24ccb309","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eebec52f"},"source":["We have a basic model up and running that demonstrates learning over the epochs, but also seeing some serious overfitting.  First step will be to adjust the architecture via transfer learning and see if a new architecture will improve performance."],"id":"eebec52f"},{"cell_type":"code","metadata":{"id":"e988b8c5","outputId":"31050086-047a-4bbe-e0bc-9a7ea21bbe12"},"source":["from tensorflow.keras.applications import mobilenet_v2\n","\n","base_model = mobilenet_v2.MobileNetV2(\n","    weights='imagenet',\n","    input_shape=(80, 80, 3),\n","    include_top=False)  \n","\n","base_model.trainable = False"],"id":"e988b8c5","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"]}]},{"cell_type":"code","metadata":{"id":"3bd3720b"},"source":["inputs = keras.Input(shape=(80, 80, 3))\n","\n","x = base_model(inputs, training=False)\n","\n","x = keras.layers.GlobalAveragePooling2D()(x)\n","\n","x = keras.layers.Dense(units=80, activation = 'relu')(x)\n","\n","outputs = keras.layers.Dense(units=40, activation = 'softmax')(x)\n","model = keras.Model(inputs, outputs)"],"id":"3bd3720b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32853c0f"},"source":["model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy'],\n",")\n","model.summary()\n","model.fit(X_train, y_train_cat, epochs=100, verbose=1, validation_split=0.25,\n","       callbacks=[\n","           keras.callbacks.ModelCheckpoint(\n","               'models/mnist.{epoch:02d}-{val_loss:.2f}.hdf5',\n","               save_best_only=True)\n","       ])  # track progress as we fit"],"id":"32853c0f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"028430c4"},"source":["mobilenetV2 performed worse than the base model, will try a more robust model, Xception"],"id":"028430c4"},{"cell_type":"code","metadata":{"scrolled":true,"id":"7fa48b9d","executionInfo":{"status":"ok","timestamp":1638484559686,"user_tz":480,"elapsed":1704,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["from tensorflow.keras.applications import Xception\n","path='/content/models/mnist.79-1.45.hdf5'\n","base_model = Xception(\n","    weights='imagenet',\n","    input_shape=(256, 256, 3),\n","    include_top=False)  \n","\n","base_model.trainable = True"],"id":"7fa48b9d","execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"07db3316","executionInfo":{"status":"ok","timestamp":1638484561275,"user_tz":480,"elapsed":556,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}}},"source":["inputs = keras.Input(shape=(256, 256, 3))\n","\n","x = base_model(inputs, training=True)\n","\n","x = keras.layers.GlobalAveragePooling2D()(x)\n","\n","x = keras.layers.Dense(units=80, activation = 'relu')(x)\n","\n","outputs = keras.layers.Dense(units=40, activation = 'softmax')(x)\n","model = keras.Model(inputs, outputs)"],"id":"07db3316","execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"bda9cf11","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638495603498,"user_tz":480,"elapsed":11039629,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"86110f73-d75f-484f-cf92-67284ccc5f0b"},"source":["#training once to create starting weights\n","model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy'],\n",")\n","model.summary()\n","model.fit(X_train, y_train_cat, epochs=100, verbose=1, validation_split=0.25,\n","       callbacks=[\n","           keras.callbacks.ModelCheckpoint(\n","               'models/mnist.{epoch:02d}-{val_loss:.2f}.hdf5',\n","               save_best_only=True)\n","       ])  # track progress as we fit"],"id":"bda9cf11","execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_14 (InputLayer)       [(None, 256, 256, 3)]     0         \n","                                                                 \n"," xception (Functional)       (None, 8, 8, 2048)        20861480  \n","                                                                 \n"," global_average_pooling2d_7   (None, 2048)             0         \n"," (GlobalAveragePooling2D)                                        \n","                                                                 \n"," dense_14 (Dense)            (None, 80)                163920    \n","                                                                 \n"," dense_15 (Dense)            (None, 40)                3240      \n","                                                                 \n","=================================================================\n","Total params: 21,028,640\n","Trainable params: 20,974,112\n","Non-trainable params: 54,528\n","_________________________________________________________________\n","Epoch 1/100\n","62/62 [==============================] - ETA: 0s - loss: 2.7643 - accuracy: 0.2586"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r62/62 [==============================] - 118s 2s/step - loss: 2.7643 - accuracy: 0.2586 - val_loss: 1.9690 - val_accuracy: 0.4158\n","Epoch 2/100\n","62/62 [==============================] - 112s 2s/step - loss: 1.6758 - accuracy: 0.4873 - val_loss: 1.5764 - val_accuracy: 0.5083\n","Epoch 3/100\n","62/62 [==============================] - 112s 2s/step - loss: 1.3170 - accuracy: 0.5876 - val_loss: 1.4972 - val_accuracy: 0.5357\n","Epoch 4/100\n","62/62 [==============================] - 112s 2s/step - loss: 1.1064 - accuracy: 0.6478 - val_loss: 1.2971 - val_accuracy: 0.6282\n","Epoch 5/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.8763 - accuracy: 0.7039 - val_loss: 1.2553 - val_accuracy: 0.6100\n","Epoch 6/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.7920 - accuracy: 0.7348 - val_loss: 1.2592 - val_accuracy: 0.6525\n","Epoch 7/100\n","62/62 [==============================] - 112s 2s/step - loss: 0.6798 - accuracy: 0.7702 - val_loss: 1.2506 - val_accuracy: 0.6328\n","Epoch 8/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.6414 - accuracy: 0.7925 - val_loss: 1.2608 - val_accuracy: 0.6540\n","Epoch 9/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.5507 - accuracy: 0.8274 - val_loss: 1.2586 - val_accuracy: 0.6646\n","Epoch 10/100\n","62/62 [==============================] - 112s 2s/step - loss: 0.4732 - accuracy: 0.8467 - val_loss: 1.1558 - val_accuracy: 0.6798\n","Epoch 11/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.4373 - accuracy: 0.8517 - val_loss: 1.1993 - val_accuracy: 0.7026\n","Epoch 12/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.4276 - accuracy: 0.8543 - val_loss: 1.3163 - val_accuracy: 0.6616\n","Epoch 13/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.3490 - accuracy: 0.8877 - val_loss: 1.2190 - val_accuracy: 0.7026\n","Epoch 14/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.2787 - accuracy: 0.9119 - val_loss: 1.1891 - val_accuracy: 0.6980\n","Epoch 15/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.2385 - accuracy: 0.9231 - val_loss: 1.4119 - val_accuracy: 0.6692\n","Epoch 16/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.2790 - accuracy: 0.9114 - val_loss: 1.3421 - val_accuracy: 0.6813\n","Epoch 17/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.2094 - accuracy: 0.9403 - val_loss: 1.3083 - val_accuracy: 0.7071\n","Epoch 18/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.1574 - accuracy: 0.9469 - val_loss: 1.2404 - val_accuracy: 0.7011\n","Epoch 19/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.2910 - accuracy: 0.9109 - val_loss: 1.2574 - val_accuracy: 0.7011\n","Epoch 20/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.2201 - accuracy: 0.9342 - val_loss: 1.4341 - val_accuracy: 0.6813\n","Epoch 21/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.2079 - accuracy: 0.9342 - val_loss: 1.3210 - val_accuracy: 0.6995\n","Epoch 22/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.1474 - accuracy: 0.9534 - val_loss: 1.3423 - val_accuracy: 0.6950\n","Epoch 23/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.1077 - accuracy: 0.9620 - val_loss: 1.4708 - val_accuracy: 0.7041\n","Epoch 24/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.2237 - accuracy: 0.9317 - val_loss: 1.3078 - val_accuracy: 0.7026\n","Epoch 25/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.2220 - accuracy: 0.9322 - val_loss: 1.4547 - val_accuracy: 0.6980\n","Epoch 26/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.1644 - accuracy: 0.9499 - val_loss: 1.4599 - val_accuracy: 0.6950\n","Epoch 27/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.1396 - accuracy: 0.9555 - val_loss: 1.5740 - val_accuracy: 0.6874\n","Epoch 28/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.1769 - accuracy: 0.9519 - val_loss: 1.4595 - val_accuracy: 0.7026\n","Epoch 29/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0964 - accuracy: 0.9732 - val_loss: 1.3927 - val_accuracy: 0.7117\n","Epoch 30/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0971 - accuracy: 0.9701 - val_loss: 1.4983 - val_accuracy: 0.7041\n","Epoch 31/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0692 - accuracy: 0.9803 - val_loss: 1.4363 - val_accuracy: 0.7223\n","Epoch 32/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0477 - accuracy: 0.9848 - val_loss: 1.4755 - val_accuracy: 0.7238\n","Epoch 33/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0606 - accuracy: 0.9863 - val_loss: 1.4908 - val_accuracy: 0.7405\n","Epoch 34/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0965 - accuracy: 0.9732 - val_loss: 1.5558 - val_accuracy: 0.7132\n","Epoch 35/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0972 - accuracy: 0.9712 - val_loss: 1.5604 - val_accuracy: 0.7011\n","Epoch 36/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.1605 - accuracy: 0.9494 - val_loss: 1.6368 - val_accuracy: 0.6920\n","Epoch 37/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.1795 - accuracy: 0.9494 - val_loss: 1.6083 - val_accuracy: 0.6707\n","Epoch 38/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0958 - accuracy: 0.9742 - val_loss: 1.6860 - val_accuracy: 0.6753\n","Epoch 39/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0811 - accuracy: 0.9798 - val_loss: 1.5024 - val_accuracy: 0.7086\n","Epoch 40/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0369 - accuracy: 0.9919 - val_loss: 1.5705 - val_accuracy: 0.7132\n","Epoch 41/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0884 - accuracy: 0.9701 - val_loss: 1.6710 - val_accuracy: 0.7041\n","Epoch 42/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.1029 - accuracy: 0.9686 - val_loss: 1.6495 - val_accuracy: 0.6920\n","Epoch 43/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0840 - accuracy: 0.9752 - val_loss: 1.6495 - val_accuracy: 0.7026\n","Epoch 44/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0678 - accuracy: 0.9787 - val_loss: 1.8723 - val_accuracy: 0.6829\n","Epoch 45/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0540 - accuracy: 0.9863 - val_loss: 1.7118 - val_accuracy: 0.7041\n","Epoch 46/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0847 - accuracy: 0.9747 - val_loss: 1.9226 - val_accuracy: 0.6753\n","Epoch 47/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.1053 - accuracy: 0.9737 - val_loss: 1.7956 - val_accuracy: 0.6768\n","Epoch 48/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.1369 - accuracy: 0.9565 - val_loss: 1.7683 - val_accuracy: 0.6935\n","Epoch 49/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.1652 - accuracy: 0.9499 - val_loss: 1.7127 - val_accuracy: 0.6920\n","Epoch 50/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0909 - accuracy: 0.9767 - val_loss: 1.6971 - val_accuracy: 0.6950\n","Epoch 51/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0893 - accuracy: 0.9762 - val_loss: 1.6186 - val_accuracy: 0.7071\n","Epoch 52/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0438 - accuracy: 0.9904 - val_loss: 1.6512 - val_accuracy: 0.7238\n","Epoch 53/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0376 - accuracy: 0.9884 - val_loss: 1.7612 - val_accuracy: 0.7086\n","Epoch 54/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0519 - accuracy: 0.9843 - val_loss: 1.6563 - val_accuracy: 0.6995\n","Epoch 55/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.1110 - accuracy: 0.9691 - val_loss: 1.5806 - val_accuracy: 0.6950\n","Epoch 56/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0771 - accuracy: 0.9747 - val_loss: 1.5658 - val_accuracy: 0.7162\n","Epoch 57/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0689 - accuracy: 0.9833 - val_loss: 1.6978 - val_accuracy: 0.6904\n","Epoch 58/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0527 - accuracy: 0.9833 - val_loss: 1.7988 - val_accuracy: 0.6935\n","Epoch 59/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0446 - accuracy: 0.9873 - val_loss: 1.7150 - val_accuracy: 0.6995\n","Epoch 60/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0812 - accuracy: 0.9803 - val_loss: 1.8445 - val_accuracy: 0.6677\n","Epoch 61/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0746 - accuracy: 0.9782 - val_loss: 1.7514 - val_accuracy: 0.6995\n","Epoch 62/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0661 - accuracy: 0.9818 - val_loss: 2.0625 - val_accuracy: 0.6571\n","Epoch 63/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0747 - accuracy: 0.9803 - val_loss: 1.5723 - val_accuracy: 0.7041\n","Epoch 64/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0872 - accuracy: 0.9772 - val_loss: 1.7756 - val_accuracy: 0.6904\n","Epoch 65/100\n","62/62 [==============================] - 109s 2s/step - loss: 0.0613 - accuracy: 0.9818 - val_loss: 1.6308 - val_accuracy: 0.7071\n","Epoch 66/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0357 - accuracy: 0.9879 - val_loss: 1.6136 - val_accuracy: 0.7193\n","Epoch 67/100\n","62/62 [==============================] - 109s 2s/step - loss: 0.0528 - accuracy: 0.9843 - val_loss: 1.7203 - val_accuracy: 0.7086\n","Epoch 68/100\n","62/62 [==============================] - 109s 2s/step - loss: 0.0155 - accuracy: 0.9975 - val_loss: 1.8489 - val_accuracy: 0.7056\n","Epoch 69/100\n","62/62 [==============================] - 109s 2s/step - loss: 0.0256 - accuracy: 0.9919 - val_loss: 1.7331 - val_accuracy: 0.7026\n","Epoch 70/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0358 - accuracy: 0.9909 - val_loss: 1.8283 - val_accuracy: 0.7041\n","Epoch 71/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0538 - accuracy: 0.9858 - val_loss: 2.0146 - val_accuracy: 0.7011\n","Epoch 72/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0816 - accuracy: 0.9803 - val_loss: 1.7004 - val_accuracy: 0.7026\n","Epoch 73/100\n","62/62 [==============================] - 109s 2s/step - loss: 0.0695 - accuracy: 0.9793 - val_loss: 1.5247 - val_accuracy: 0.7011\n","Epoch 74/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0599 - accuracy: 0.9808 - val_loss: 1.7393 - val_accuracy: 0.7026\n","Epoch 75/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 1.8382 - val_accuracy: 0.7147\n","Epoch 76/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0236 - accuracy: 0.9954 - val_loss: 2.0198 - val_accuracy: 0.6904\n","Epoch 77/100\n","62/62 [==============================] - 109s 2s/step - loss: 0.0576 - accuracy: 0.9863 - val_loss: 1.9538 - val_accuracy: 0.6692\n","Epoch 78/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.1110 - accuracy: 0.9706 - val_loss: 1.8761 - val_accuracy: 0.6783\n","Epoch 79/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0907 - accuracy: 0.9762 - val_loss: 1.6364 - val_accuracy: 0.6920\n","Epoch 80/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0279 - accuracy: 0.9914 - val_loss: 1.7972 - val_accuracy: 0.7132\n","Epoch 81/100\n","62/62 [==============================] - 109s 2s/step - loss: 0.0064 - accuracy: 0.9990 - val_loss: 1.8082 - val_accuracy: 0.7132\n","Epoch 82/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0103 - accuracy: 0.9975 - val_loss: 1.9563 - val_accuracy: 0.7056\n","Epoch 83/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0670 - accuracy: 0.9813 - val_loss: 1.9092 - val_accuracy: 0.6874\n","Epoch 84/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0950 - accuracy: 0.9706 - val_loss: 1.7099 - val_accuracy: 0.6950\n","Epoch 85/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0733 - accuracy: 0.9777 - val_loss: 1.7690 - val_accuracy: 0.7162\n","Epoch 86/100\n","62/62 [==============================] - 111s 2s/step - loss: 0.0864 - accuracy: 0.9747 - val_loss: 1.8615 - val_accuracy: 0.6980\n","Epoch 87/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0827 - accuracy: 0.9696 - val_loss: 1.8109 - val_accuracy: 0.6829\n","Epoch 88/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0336 - accuracy: 0.9904 - val_loss: 1.6975 - val_accuracy: 0.6950\n","Epoch 89/100\n","62/62 [==============================] - 109s 2s/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 1.6123 - val_accuracy: 0.7132\n","Epoch 90/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0582 - accuracy: 0.9838 - val_loss: 1.7284 - val_accuracy: 0.7056\n","Epoch 91/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0271 - accuracy: 0.9924 - val_loss: 1.8585 - val_accuracy: 0.7117\n","Epoch 92/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0128 - accuracy: 0.9960 - val_loss: 1.8878 - val_accuracy: 0.7011\n","Epoch 93/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0137 - accuracy: 0.9960 - val_loss: 2.0049 - val_accuracy: 0.7086\n","Epoch 94/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0426 - accuracy: 0.9868 - val_loss: 2.0212 - val_accuracy: 0.6965\n","Epoch 95/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0604 - accuracy: 0.9818 - val_loss: 1.9601 - val_accuracy: 0.7026\n","Epoch 96/100\n","62/62 [==============================] - 109s 2s/step - loss: 0.0542 - accuracy: 0.9858 - val_loss: 1.8740 - val_accuracy: 0.6737\n","Epoch 97/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0485 - accuracy: 0.9858 - val_loss: 1.8161 - val_accuracy: 0.6844\n","Epoch 98/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0248 - accuracy: 0.9929 - val_loss: 1.9178 - val_accuracy: 0.7026\n","Epoch 99/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0407 - accuracy: 0.9894 - val_loss: 1.8184 - val_accuracy: 0.6844\n","Epoch 100/100\n","62/62 [==============================] - 110s 2s/step - loss: 0.0636 - accuracy: 0.9823 - val_loss: 1.6797 - val_accuracy: 0.7162\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f528377e2d0>"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"qLlLqdBdi1Zp"},"source":["from sklearn.metrics import accuracy_score\n","\n","preds = np.argmax(NN.predict(X_test), axis=1)\n","accuracy_score(y_test, preds)"],"id":"qLlLqdBdi1Zp","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2326bab9"},"source":["Going back to the base model, and attempting regularization in the dense layer to try and reduce overfitting"],"id":"2326bab9"},{"cell_type":"code","metadata":{"id":"5d508824","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638477974493,"user_tz":480,"elapsed":803123,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"fa594648-e572-48e7-86a8-59b251bd1c95"},"source":["from tensorflow.keras import regularizers\n","\n","NN = Sequential()\n","\n","NN.add(InputLayer(input_shape=X_train.shape[1:]))\n","\n","\n","NN.add(Conv2D(filters=30, kernel_size=3, activation='relu', padding='same'))\n","\n","NN.add(MaxPooling2D())\n","\n","\n","NN.add(Conv2D(filters=60, kernel_size=3, activation='relu', padding='same'))\n","\n","NN.add(MaxPooling2D())\n","\n","\n","NN.add(Conv2D(filters=90, kernel_size=3, activation='relu', padding='same'))\n","\n","NN.add(GlobalAveragePooling2D())\n","\n","\n","NN.add(Flatten())\n","\n","NN.add(Dense(80, activation='relu', kernel_regularizer='l2'))#added l2 regularizer here\n","\n","NN.add(Dense(40, activation='softmax'))  # 40 target classes\n","\n","NN.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy'],\n",")\n","NN.summary()\n","NN.fit(X_train, y_train_cat, epochs=100, verbose=1, validation_split=0.25,\n","       callbacks=[\n","           keras.callbacks.ModelCheckpoint(\n","               'models/mnist.{epoch:02d}-{val_loss:.2f}.hdf5',\n","               save_best_only=True)\n","       ])  # track progress as we fit"],"id":"5d508824","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_9 (Conv2D)           (None, 256, 256, 30)      840       \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 128, 128, 30)     0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 128, 128, 60)      16260     \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 64, 64, 60)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 64, 64, 90)        48690     \n","                                                                 \n"," global_average_pooling2d_3   (None, 90)               0         \n"," (GlobalAveragePooling2D)                                        \n","                                                                 \n"," flatten_3 (Flatten)         (None, 90)                0         \n","                                                                 \n"," dense_6 (Dense)             (None, 80)                7280      \n","                                                                 \n"," dense_7 (Dense)             (None, 40)                3240      \n","                                                                 \n","=================================================================\n","Total params: 76,310\n","Trainable params: 76,310\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","62/62 [==============================] - 9s 129ms/step - loss: 9.5066 - accuracy: 0.0299 - val_loss: 4.3262 - val_accuracy: 0.0440\n","Epoch 2/100\n","62/62 [==============================] - 8s 123ms/step - loss: 4.1595 - accuracy: 0.0466 - val_loss: 3.9768 - val_accuracy: 0.0804\n","Epoch 3/100\n","62/62 [==============================] - 8s 122ms/step - loss: 3.8373 - accuracy: 0.0982 - val_loss: 3.6247 - val_accuracy: 0.1396\n","Epoch 4/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.5682 - accuracy: 0.1341 - val_loss: 3.4118 - val_accuracy: 0.1639\n","Epoch 5/100\n","62/62 [==============================] - 8s 122ms/step - loss: 3.3553 - accuracy: 0.1726 - val_loss: 3.2155 - val_accuracy: 0.1912\n","Epoch 6/100\n","62/62 [==============================] - 8s 122ms/step - loss: 3.1995 - accuracy: 0.2024 - val_loss: 3.0399 - val_accuracy: 0.2640\n","Epoch 7/100\n","62/62 [==============================] - 8s 122ms/step - loss: 3.1065 - accuracy: 0.2257 - val_loss: 3.0220 - val_accuracy: 0.2109\n","Epoch 8/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.9515 - accuracy: 0.2505 - val_loss: 2.8892 - val_accuracy: 0.2747\n","Epoch 9/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.8647 - accuracy: 0.2864 - val_loss: 2.9127 - val_accuracy: 0.2473\n","Epoch 10/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.7359 - accuracy: 0.3122 - val_loss: 2.7849 - val_accuracy: 0.2974\n","Epoch 11/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.6455 - accuracy: 0.3289 - val_loss: 2.6383 - val_accuracy: 0.3217\n","Epoch 12/100\n","62/62 [==============================] - 8s 132ms/step - loss: 2.5410 - accuracy: 0.3502 - val_loss: 2.7739 - val_accuracy: 0.2898\n","Epoch 13/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.4218 - accuracy: 0.3942 - val_loss: 2.5223 - val_accuracy: 0.3794\n","Epoch 14/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.3044 - accuracy: 0.4140 - val_loss: 2.4619 - val_accuracy: 0.4036\n","Epoch 15/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.2466 - accuracy: 0.4327 - val_loss: 2.4076 - val_accuracy: 0.4006\n","Epoch 16/100\n","62/62 [==============================] - 8s 121ms/step - loss: 2.1733 - accuracy: 0.4550 - val_loss: 2.5765 - val_accuracy: 0.3672\n","Epoch 17/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.1326 - accuracy: 0.4691 - val_loss: 2.3698 - val_accuracy: 0.4143\n","Epoch 18/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.0354 - accuracy: 0.4954 - val_loss: 2.2805 - val_accuracy: 0.4294\n","Epoch 19/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.9948 - accuracy: 0.5076 - val_loss: 2.2717 - val_accuracy: 0.4507\n","Epoch 20/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.9453 - accuracy: 0.5157 - val_loss: 2.3007 - val_accuracy: 0.4431\n","Epoch 21/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.9246 - accuracy: 0.5101 - val_loss: 2.2630 - val_accuracy: 0.4659\n","Epoch 22/100\n","62/62 [==============================] - 8s 124ms/step - loss: 1.8415 - accuracy: 0.5364 - val_loss: 2.0812 - val_accuracy: 0.4901\n","Epoch 23/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.8414 - accuracy: 0.5223 - val_loss: 2.2085 - val_accuracy: 0.4810\n","Epoch 24/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.7905 - accuracy: 0.5471 - val_loss: 2.1220 - val_accuracy: 0.4932\n","Epoch 25/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.7442 - accuracy: 0.5577 - val_loss: 2.0621 - val_accuracy: 0.4947\n","Epoch 26/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.7213 - accuracy: 0.5587 - val_loss: 2.0271 - val_accuracy: 0.5311\n","Epoch 27/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.6675 - accuracy: 0.5810 - val_loss: 2.0035 - val_accuracy: 0.5524\n","Epoch 28/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.6350 - accuracy: 0.5886 - val_loss: 1.9614 - val_accuracy: 0.5432\n","Epoch 29/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.6330 - accuracy: 0.5870 - val_loss: 1.9396 - val_accuracy: 0.5432\n","Epoch 30/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.5923 - accuracy: 0.5916 - val_loss: 2.0940 - val_accuracy: 0.5068\n","Epoch 31/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.5868 - accuracy: 0.6007 - val_loss: 1.9189 - val_accuracy: 0.5493\n","Epoch 32/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.4971 - accuracy: 0.6260 - val_loss: 1.9047 - val_accuracy: 0.5311\n","Epoch 33/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.4823 - accuracy: 0.6250 - val_loss: 1.8335 - val_accuracy: 0.5630\n","Epoch 34/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.4718 - accuracy: 0.6255 - val_loss: 1.8342 - val_accuracy: 0.5721\n","Epoch 35/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.4421 - accuracy: 0.6290 - val_loss: 1.8391 - val_accuracy: 0.5630\n","Epoch 36/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.4607 - accuracy: 0.6159 - val_loss: 1.7968 - val_accuracy: 0.5812\n","Epoch 37/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.3800 - accuracy: 0.6498 - val_loss: 1.7923 - val_accuracy: 0.5660\n","Epoch 38/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.3522 - accuracy: 0.6523 - val_loss: 1.7267 - val_accuracy: 0.5918\n","Epoch 39/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.3506 - accuracy: 0.6604 - val_loss: 1.6968 - val_accuracy: 0.5645\n","Epoch 40/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.3501 - accuracy: 0.6488 - val_loss: 1.7293 - val_accuracy: 0.5827\n","Epoch 41/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.3693 - accuracy: 0.6422 - val_loss: 1.6659 - val_accuracy: 0.6070\n","Epoch 42/100\n","62/62 [==============================] - 8s 121ms/step - loss: 1.3061 - accuracy: 0.6478 - val_loss: 1.9082 - val_accuracy: 0.5539\n","Epoch 43/100\n","62/62 [==============================] - 7s 121ms/step - loss: 1.3040 - accuracy: 0.6736 - val_loss: 1.7929 - val_accuracy: 0.5706\n","Epoch 44/100\n","62/62 [==============================] - 8s 121ms/step - loss: 1.3134 - accuracy: 0.6594 - val_loss: 1.8126 - val_accuracy: 0.5584\n","Epoch 45/100\n","62/62 [==============================] - 7s 121ms/step - loss: 1.2786 - accuracy: 0.6655 - val_loss: 1.7182 - val_accuracy: 0.5797\n","Epoch 46/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.2571 - accuracy: 0.6695 - val_loss: 1.6438 - val_accuracy: 0.6161\n","Epoch 47/100\n","62/62 [==============================] - 8s 121ms/step - loss: 1.2256 - accuracy: 0.6883 - val_loss: 1.7188 - val_accuracy: 0.5964\n","Epoch 48/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.2199 - accuracy: 0.6847 - val_loss: 1.6763 - val_accuracy: 0.6100\n","Epoch 49/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.1894 - accuracy: 0.6878 - val_loss: 1.6006 - val_accuracy: 0.6085\n","Epoch 50/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.1355 - accuracy: 0.7024 - val_loss: 1.7127 - val_accuracy: 0.5812\n","Epoch 51/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.1395 - accuracy: 0.6974 - val_loss: 1.6653 - val_accuracy: 0.6115\n","Epoch 52/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.1612 - accuracy: 0.7095 - val_loss: 1.6076 - val_accuracy: 0.6282\n","Epoch 53/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.1014 - accuracy: 0.7181 - val_loss: 1.5970 - val_accuracy: 0.6191\n","Epoch 54/100\n","62/62 [==============================] - 7s 121ms/step - loss: 1.1018 - accuracy: 0.7060 - val_loss: 1.5792 - val_accuracy: 0.6267\n","Epoch 55/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0838 - accuracy: 0.7156 - val_loss: 1.6044 - val_accuracy: 0.6191\n","Epoch 56/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0728 - accuracy: 0.7126 - val_loss: 1.7682 - val_accuracy: 0.5948\n","Epoch 57/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0958 - accuracy: 0.7050 - val_loss: 1.5933 - val_accuracy: 0.6297\n","Epoch 58/100\n","62/62 [==============================] - 8s 121ms/step - loss: 1.0876 - accuracy: 0.7181 - val_loss: 1.6578 - val_accuracy: 0.6024\n","Epoch 59/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0667 - accuracy: 0.7237 - val_loss: 1.6695 - val_accuracy: 0.6009\n","Epoch 60/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.0360 - accuracy: 0.7368 - val_loss: 1.6024 - val_accuracy: 0.6297\n","Epoch 61/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0195 - accuracy: 0.7358 - val_loss: 1.6723 - val_accuracy: 0.6039\n","Epoch 62/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.0600 - accuracy: 0.7070 - val_loss: 1.6238 - val_accuracy: 0.6055\n","Epoch 63/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.0251 - accuracy: 0.7247 - val_loss: 1.6287 - val_accuracy: 0.6267\n","Epoch 64/100\n","62/62 [==============================] - 8s 124ms/step - loss: 1.0734 - accuracy: 0.7090 - val_loss: 1.7473 - val_accuracy: 0.5630\n","Epoch 65/100\n","62/62 [==============================] - 8s 132ms/step - loss: 1.0299 - accuracy: 0.7161 - val_loss: 1.5057 - val_accuracy: 0.6328\n","Epoch 66/100\n","62/62 [==============================] - 8s 124ms/step - loss: 0.9644 - accuracy: 0.7358 - val_loss: 1.5905 - val_accuracy: 0.6388\n","Epoch 67/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.9198 - accuracy: 0.7662 - val_loss: 1.5540 - val_accuracy: 0.6373\n","Epoch 68/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.9657 - accuracy: 0.7439 - val_loss: 1.6134 - val_accuracy: 0.6176\n","Epoch 69/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.9307 - accuracy: 0.7591 - val_loss: 1.5510 - val_accuracy: 0.6419\n","Epoch 70/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.9477 - accuracy: 0.7495 - val_loss: 1.5718 - val_accuracy: 0.6358\n","Epoch 71/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.9053 - accuracy: 0.7672 - val_loss: 1.6303 - val_accuracy: 0.6434\n","Epoch 72/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.8906 - accuracy: 0.7637 - val_loss: 1.6141 - val_accuracy: 0.6131\n","Epoch 73/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.9058 - accuracy: 0.7687 - val_loss: 1.7496 - val_accuracy: 0.6146\n","Epoch 74/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8539 - accuracy: 0.7809 - val_loss: 1.6665 - val_accuracy: 0.6267\n","Epoch 75/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8594 - accuracy: 0.7728 - val_loss: 1.5307 - val_accuracy: 0.6419\n","Epoch 76/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.8949 - accuracy: 0.7632 - val_loss: 1.6606 - val_accuracy: 0.6131\n","Epoch 77/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.9028 - accuracy: 0.7667 - val_loss: 1.5994 - val_accuracy: 0.6525\n","Epoch 78/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.8570 - accuracy: 0.7844 - val_loss: 1.5854 - val_accuracy: 0.6343\n","Epoch 79/100\n","62/62 [==============================] - 8s 133ms/step - loss: 0.8921 - accuracy: 0.7540 - val_loss: 1.4508 - val_accuracy: 0.6844\n","Epoch 80/100\n","62/62 [==============================] - 8s 124ms/step - loss: 0.8457 - accuracy: 0.7606 - val_loss: 1.6587 - val_accuracy: 0.6222\n","Epoch 81/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.7928 - accuracy: 0.7935 - val_loss: 1.5835 - val_accuracy: 0.6358\n","Epoch 82/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.7951 - accuracy: 0.7925 - val_loss: 1.6104 - val_accuracy: 0.6449\n","Epoch 83/100\n","62/62 [==============================] - 8s 132ms/step - loss: 0.8339 - accuracy: 0.7844 - val_loss: 1.5747 - val_accuracy: 0.6343\n","Epoch 84/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7754 - accuracy: 0.7996 - val_loss: 1.5920 - val_accuracy: 0.6480\n","Epoch 85/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8125 - accuracy: 0.7864 - val_loss: 1.5689 - val_accuracy: 0.6601\n","Epoch 86/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.7504 - accuracy: 0.8138 - val_loss: 1.4458 - val_accuracy: 0.6555\n","Epoch 87/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7384 - accuracy: 0.8148 - val_loss: 1.5283 - val_accuracy: 0.6601\n","Epoch 88/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.7384 - accuracy: 0.8128 - val_loss: 1.5478 - val_accuracy: 0.6783\n","Epoch 89/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7650 - accuracy: 0.8016 - val_loss: 1.6109 - val_accuracy: 0.6495\n","Epoch 90/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.8221 - accuracy: 0.7753 - val_loss: 1.5549 - val_accuracy: 0.6540\n","Epoch 91/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7146 - accuracy: 0.8209 - val_loss: 1.6295 - val_accuracy: 0.6388\n","Epoch 92/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7795 - accuracy: 0.7864 - val_loss: 1.6121 - val_accuracy: 0.6404\n","Epoch 93/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.7193 - accuracy: 0.8138 - val_loss: 1.5380 - val_accuracy: 0.6753\n","Epoch 94/100\n","62/62 [==============================] - 8s 133ms/step - loss: 0.8042 - accuracy: 0.7950 - val_loss: 1.8428 - val_accuracy: 0.5888\n","Epoch 95/100\n","62/62 [==============================] - 8s 123ms/step - loss: 0.8530 - accuracy: 0.7713 - val_loss: 1.6861 - val_accuracy: 0.6480\n","Epoch 96/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.7392 - accuracy: 0.8122 - val_loss: 1.6065 - val_accuracy: 0.6631\n","Epoch 97/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.7016 - accuracy: 0.8259 - val_loss: 1.5910 - val_accuracy: 0.6646\n","Epoch 98/100\n","62/62 [==============================] - 8s 121ms/step - loss: 0.7293 - accuracy: 0.8072 - val_loss: 1.7552 - val_accuracy: 0.6328\n","Epoch 99/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.6874 - accuracy: 0.8209 - val_loss: 1.5314 - val_accuracy: 0.6707\n","Epoch 100/100\n","62/62 [==============================] - 8s 122ms/step - loss: 0.6810 - accuracy: 0.8305 - val_loss: 1.5663 - val_accuracy: 0.6646\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5286352d90>"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"f11fb927"},"source":["accuracy much lower, setting more patience for callback, adding ReduceLROnPlateau callback"],"id":"f11fb927"},{"cell_type":"code","metadata":{"id":"fcc0d45f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638476784333,"user_tz":480,"elapsed":621378,"user":{"displayName":"Alika Ziker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01545907986383780605"}},"outputId":"d27c747d-8ac6-4e63-efe5-6e1198f2c045"},"source":["from tensorflow.keras.layers import Dropout\n","\n","NN = Sequential()\n","\n","NN.add(InputLayer(input_shape=X_train.shape[1:]))\n","\n","\n","NN.add(Conv2D(filters=30, kernel_size=3, activation='relu', padding='same'))\n","\n","NN.add(MaxPooling2D())\n","\n","\n","NN.add(Conv2D(filters=60, kernel_size=3, activation='relu', padding='same'))\n","\n","NN.add(MaxPooling2D())\n","\n","\n","NN.add(Conv2D(filters=90, kernel_size=3, activation='relu', padding='same'))\n","\n","NN.add(GlobalAveragePooling2D())\n","\n","\n","NN.add(Flatten())\n","\n","NN.add(Dense(80, activation='relu', kernel_regularizer='l2'))\n","\n","#NN.add(Dropout(0.05)) #added dropout here\n","\n","NN.add(Dense(40, activation='softmax'))  # 40 target classes\n","\n","NN.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy'],\n",")\n","NN.summary()\n","NN.fit(X_train, y_train_cat, epochs=100, verbose=1, validation_split=0.25,\n","       callbacks=[\n","           keras.callbacks.ModelCheckpoint(\n","               'models/mnist.{epoch:02d}-{val_loss:.2f}.hdf5',\n","               save_best_only=True),\n","           keras.callbacks.ReduceLROnPlateau(\n","               factor=.5,\n","               patience=3,\n","               verbose=1),\n","           keras.callbacks.EarlyStopping(\n","               patience=8,\n","               verbose=1,\n","               restore_best_weights=True)\n","       ])  # track progress as we fit"],"id":"fcc0d45f","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_6 (Conv2D)           (None, 256, 256, 30)      840       \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 128, 128, 30)     0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 128, 128, 60)      16260     \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 64, 64, 60)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 64, 64, 90)        48690     \n","                                                                 \n"," global_average_pooling2d_2   (None, 90)               0         \n"," (GlobalAveragePooling2D)                                        \n","                                                                 \n"," flatten_2 (Flatten)         (None, 90)                0         \n","                                                                 \n"," dense_4 (Dense)             (None, 80)                7280      \n","                                                                 \n"," dense_5 (Dense)             (None, 40)                3240      \n","                                                                 \n","=================================================================\n","Total params: 76,310\n","Trainable params: 76,310\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","62/62 [==============================] - 9s 129ms/step - loss: 5.5728 - accuracy: 0.0349 - val_loss: 4.2686 - val_accuracy: 0.0228 - lr: 0.0010\n","Epoch 2/100\n","62/62 [==============================] - 8s 123ms/step - loss: 4.1475 - accuracy: 0.0294 - val_loss: 4.1320 - val_accuracy: 0.0334 - lr: 0.0010\n","Epoch 3/100\n","62/62 [==============================] - 8s 123ms/step - loss: 4.0143 - accuracy: 0.0481 - val_loss: 3.9450 - val_accuracy: 0.0637 - lr: 0.0010\n","Epoch 4/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.8590 - accuracy: 0.0562 - val_loss: 3.8002 - val_accuracy: 0.0759 - lr: 0.0010\n","Epoch 5/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.7258 - accuracy: 0.0850 - val_loss: 3.6154 - val_accuracy: 0.1123 - lr: 0.0010\n","Epoch 6/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.5416 - accuracy: 0.0992 - val_loss: 3.4753 - val_accuracy: 0.1320 - lr: 0.0010\n","Epoch 7/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.3823 - accuracy: 0.1215 - val_loss: 3.3070 - val_accuracy: 0.1457 - lr: 0.0010\n","Epoch 8/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.2677 - accuracy: 0.1346 - val_loss: 3.1474 - val_accuracy: 0.1821 - lr: 0.0010\n","Epoch 9/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.1331 - accuracy: 0.1457 - val_loss: 3.0394 - val_accuracy: 0.1760 - lr: 0.0010\n","Epoch 10/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.0513 - accuracy: 0.1695 - val_loss: 3.0297 - val_accuracy: 0.1806 - lr: 0.0010\n","Epoch 11/100\n","62/62 [==============================] - 8s 123ms/step - loss: 3.0020 - accuracy: 0.1741 - val_loss: 3.0482 - val_accuracy: 0.1578 - lr: 0.0010\n","Epoch 12/100\n","62/62 [==============================] - 8s 133ms/step - loss: 2.9442 - accuracy: 0.1969 - val_loss: 2.8787 - val_accuracy: 0.2049 - lr: 0.0010\n","Epoch 13/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.8788 - accuracy: 0.1964 - val_loss: 2.8301 - val_accuracy: 0.2307 - lr: 0.0010\n","Epoch 14/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.8094 - accuracy: 0.2206 - val_loss: 2.7774 - val_accuracy: 0.2382 - lr: 0.0010\n","Epoch 15/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.7338 - accuracy: 0.2414 - val_loss: 2.7075 - val_accuracy: 0.2489 - lr: 0.0010\n","Epoch 16/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.6360 - accuracy: 0.2591 - val_loss: 2.7122 - val_accuracy: 0.2534 - lr: 0.0010\n","Epoch 17/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.5850 - accuracy: 0.2662 - val_loss: 2.5556 - val_accuracy: 0.2914 - lr: 0.0010\n","Epoch 18/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.5105 - accuracy: 0.2920 - val_loss: 2.4837 - val_accuracy: 0.3399 - lr: 0.0010\n","Epoch 19/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.4164 - accuracy: 0.3249 - val_loss: 2.4746 - val_accuracy: 0.3065 - lr: 0.0010\n","Epoch 20/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.3451 - accuracy: 0.3381 - val_loss: 2.4085 - val_accuracy: 0.3551 - lr: 0.0010\n","Epoch 21/100\n","62/62 [==============================] - 8s 132ms/step - loss: 2.2587 - accuracy: 0.3745 - val_loss: 2.3727 - val_accuracy: 0.3520 - lr: 0.0010\n","Epoch 22/100\n","62/62 [==============================] - 8s 123ms/step - loss: 2.2209 - accuracy: 0.3806 - val_loss: 2.3306 - val_accuracy: 0.3703 - lr: 0.0010\n","Epoch 23/100\n","62/62 [==============================] - 8s 132ms/step - loss: 2.1434 - accuracy: 0.4013 - val_loss: 2.2548 - val_accuracy: 0.3945 - lr: 0.0010\n","Epoch 24/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.1344 - accuracy: 0.4069 - val_loss: 2.3494 - val_accuracy: 0.3581 - lr: 0.0010\n","Epoch 25/100\n","62/62 [==============================] - 8s 122ms/step - loss: 2.0588 - accuracy: 0.4069 - val_loss: 2.3245 - val_accuracy: 0.3566 - lr: 0.0010\n","Epoch 26/100\n","62/62 [==============================] - 8s 133ms/step - loss: 2.0542 - accuracy: 0.4150 - val_loss: 2.1505 - val_accuracy: 0.4112 - lr: 0.0010\n","Epoch 27/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.9754 - accuracy: 0.4317 - val_loss: 2.1853 - val_accuracy: 0.4006 - lr: 0.0010\n","Epoch 28/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.9230 - accuracy: 0.4489 - val_loss: 2.1015 - val_accuracy: 0.4340 - lr: 0.0010\n","Epoch 29/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.9019 - accuracy: 0.4560 - val_loss: 2.0448 - val_accuracy: 0.4765 - lr: 0.0010\n","Epoch 30/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.8734 - accuracy: 0.4631 - val_loss: 2.3044 - val_accuracy: 0.3672 - lr: 0.0010\n","Epoch 31/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.8487 - accuracy: 0.4691 - val_loss: 2.0733 - val_accuracy: 0.4067 - lr: 0.0010\n","Epoch 32/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.8328 - accuracy: 0.4691 - val_loss: 2.0128 - val_accuracy: 0.4871 - lr: 0.0010\n","Epoch 33/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.7564 - accuracy: 0.5040 - val_loss: 2.0606 - val_accuracy: 0.4476 - lr: 0.0010\n","Epoch 34/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.7209 - accuracy: 0.4990 - val_loss: 1.9514 - val_accuracy: 0.4719 - lr: 0.0010\n","Epoch 35/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.7048 - accuracy: 0.4960 - val_loss: 1.9793 - val_accuracy: 0.4810 - lr: 0.0010\n","Epoch 36/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.7211 - accuracy: 0.4924 - val_loss: 1.9198 - val_accuracy: 0.5008 - lr: 0.0010\n","Epoch 37/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.6691 - accuracy: 0.5152 - val_loss: 1.9036 - val_accuracy: 0.4674 - lr: 0.0010\n","Epoch 38/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.6882 - accuracy: 0.5025 - val_loss: 1.9186 - val_accuracy: 0.4917 - lr: 0.0010\n","Epoch 39/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.6057 - accuracy: 0.5319 - val_loss: 2.0363 - val_accuracy: 0.4492 - lr: 0.0010\n","Epoch 40/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.6418 - accuracy: 0.5213 - val_loss: 1.8903 - val_accuracy: 0.4841 - lr: 0.0010\n","Epoch 41/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.5964 - accuracy: 0.5309 - val_loss: 1.8918 - val_accuracy: 0.4992 - lr: 0.0010\n","Epoch 42/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.5974 - accuracy: 0.5248 - val_loss: 1.7913 - val_accuracy: 0.5144 - lr: 0.0010\n","Epoch 43/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.5435 - accuracy: 0.5385 - val_loss: 1.8060 - val_accuracy: 0.4917 - lr: 0.0010\n","Epoch 44/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.5252 - accuracy: 0.5572 - val_loss: 1.8766 - val_accuracy: 0.5144 - lr: 0.0010\n","Epoch 45/100\n","62/62 [==============================] - ETA: 0s - loss: 1.4968 - accuracy: 0.5678\n","Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","62/62 [==============================] - 8s 123ms/step - loss: 1.4968 - accuracy: 0.5678 - val_loss: 1.9197 - val_accuracy: 0.4947 - lr: 0.0010\n","Epoch 46/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.4343 - accuracy: 0.5805 - val_loss: 1.7457 - val_accuracy: 0.5372 - lr: 5.0000e-04\n","Epoch 47/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.3967 - accuracy: 0.5946 - val_loss: 1.7823 - val_accuracy: 0.5114 - lr: 5.0000e-04\n","Epoch 48/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.4054 - accuracy: 0.5946 - val_loss: 1.7660 - val_accuracy: 0.5493 - lr: 5.0000e-04\n","Epoch 49/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.3834 - accuracy: 0.5865 - val_loss: 1.7448 - val_accuracy: 0.5311 - lr: 5.0000e-04\n","Epoch 50/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.4000 - accuracy: 0.5916 - val_loss: 1.7211 - val_accuracy: 0.5296 - lr: 5.0000e-04\n","Epoch 51/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.3545 - accuracy: 0.6139 - val_loss: 1.7834 - val_accuracy: 0.5478 - lr: 5.0000e-04\n","Epoch 52/100\n","62/62 [==============================] - 8s 124ms/step - loss: 1.3636 - accuracy: 0.5977 - val_loss: 1.6960 - val_accuracy: 0.5599 - lr: 5.0000e-04\n","Epoch 53/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.3667 - accuracy: 0.5956 - val_loss: 1.7524 - val_accuracy: 0.5326 - lr: 5.0000e-04\n","Epoch 54/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.3348 - accuracy: 0.6129 - val_loss: 1.7095 - val_accuracy: 0.5417 - lr: 5.0000e-04\n","Epoch 55/100\n","62/62 [==============================] - ETA: 0s - loss: 1.3519 - accuracy: 0.6053\n","Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","62/62 [==============================] - 8s 123ms/step - loss: 1.3519 - accuracy: 0.6053 - val_loss: 1.7077 - val_accuracy: 0.5432 - lr: 5.0000e-04\n","Epoch 56/100\n","62/62 [==============================] - 8s 124ms/step - loss: 1.2925 - accuracy: 0.6210 - val_loss: 1.6874 - val_accuracy: 0.5630 - lr: 2.5000e-04\n","Epoch 57/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.2740 - accuracy: 0.6245 - val_loss: 1.6914 - val_accuracy: 0.5630 - lr: 2.5000e-04\n","Epoch 58/100\n","62/62 [==============================] - 8s 122ms/step - loss: 1.2784 - accuracy: 0.6255 - val_loss: 1.6941 - val_accuracy: 0.5721 - lr: 2.5000e-04\n","Epoch 59/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2771 - accuracy: 0.6260 - val_loss: 1.6733 - val_accuracy: 0.5721 - lr: 2.5000e-04\n","Epoch 60/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2678 - accuracy: 0.6341 - val_loss: 1.6730 - val_accuracy: 0.5812 - lr: 2.5000e-04\n","Epoch 61/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2767 - accuracy: 0.6387 - val_loss: 1.6824 - val_accuracy: 0.5675 - lr: 2.5000e-04\n","Epoch 62/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2539 - accuracy: 0.6417 - val_loss: 1.6662 - val_accuracy: 0.5781 - lr: 2.5000e-04\n","Epoch 63/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2539 - accuracy: 0.6331 - val_loss: 1.6681 - val_accuracy: 0.5599 - lr: 2.5000e-04\n","Epoch 64/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2499 - accuracy: 0.6366 - val_loss: 1.6739 - val_accuracy: 0.5751 - lr: 2.5000e-04\n","Epoch 65/100\n","62/62 [==============================] - ETA: 0s - loss: 1.2722 - accuracy: 0.6275\n","Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2722 - accuracy: 0.6275 - val_loss: 1.6703 - val_accuracy: 0.5721 - lr: 2.5000e-04\n","Epoch 66/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2303 - accuracy: 0.6498 - val_loss: 1.6584 - val_accuracy: 0.5827 - lr: 1.2500e-04\n","Epoch 67/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2246 - accuracy: 0.6498 - val_loss: 1.6531 - val_accuracy: 0.5812 - lr: 1.2500e-04\n","Epoch 68/100\n","62/62 [==============================] - 8s 132ms/step - loss: 1.2229 - accuracy: 0.6523 - val_loss: 1.6966 - val_accuracy: 0.5660 - lr: 1.2500e-04\n","Epoch 69/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2197 - accuracy: 0.6538 - val_loss: 1.6795 - val_accuracy: 0.5751 - lr: 1.2500e-04\n","Epoch 70/100\n","62/62 [==============================] - ETA: 0s - loss: 1.2181 - accuracy: 0.6528\n","Epoch 00070: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2181 - accuracy: 0.6528 - val_loss: 1.6771 - val_accuracy: 0.5706 - lr: 1.2500e-04\n","Epoch 71/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2054 - accuracy: 0.6549 - val_loss: 1.6546 - val_accuracy: 0.5842 - lr: 6.2500e-05\n","Epoch 72/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.2023 - accuracy: 0.6614 - val_loss: 1.6621 - val_accuracy: 0.5842 - lr: 6.2500e-05\n","Epoch 73/100\n","62/62 [==============================] - 8s 124ms/step - loss: 1.2020 - accuracy: 0.6645 - val_loss: 1.6466 - val_accuracy: 0.5812 - lr: 6.2500e-05\n","Epoch 74/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.1964 - accuracy: 0.6635 - val_loss: 1.6501 - val_accuracy: 0.5766 - lr: 6.2500e-05\n","Epoch 75/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.1977 - accuracy: 0.6604 - val_loss: 1.6481 - val_accuracy: 0.5812 - lr: 6.2500e-05\n","Epoch 76/100\n","62/62 [==============================] - ETA: 0s - loss: 1.1926 - accuracy: 0.6660\n","Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","62/62 [==============================] - 8s 123ms/step - loss: 1.1926 - accuracy: 0.6660 - val_loss: 1.6650 - val_accuracy: 0.5827 - lr: 6.2500e-05\n","Epoch 77/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.1902 - accuracy: 0.6630 - val_loss: 1.6615 - val_accuracy: 0.5827 - lr: 3.1250e-05\n","Epoch 78/100\n","62/62 [==============================] - 8s 124ms/step - loss: 1.1888 - accuracy: 0.6599 - val_loss: 1.6505 - val_accuracy: 0.5857 - lr: 3.1250e-05\n","Epoch 79/100\n","62/62 [==============================] - ETA: 0s - loss: 1.1875 - accuracy: 0.6640\n","Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","62/62 [==============================] - 8s 123ms/step - loss: 1.1875 - accuracy: 0.6640 - val_loss: 1.6540 - val_accuracy: 0.5736 - lr: 3.1250e-05\n","Epoch 80/100\n","62/62 [==============================] - 8s 123ms/step - loss: 1.1853 - accuracy: 0.6650 - val_loss: 1.6528 - val_accuracy: 0.5827 - lr: 1.5625e-05\n","Epoch 81/100\n","62/62 [==============================] - ETA: 0s - loss: 1.1845 - accuracy: 0.6675Restoring model weights from the end of the best epoch: 73.\n","62/62 [==============================] - 8s 124ms/step - loss: 1.1845 - accuracy: 0.6675 - val_loss: 1.6537 - val_accuracy: 0.5812 - lr: 1.5625e-05\n","Epoch 00081: early stopping\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f520e0eb510>"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"398910c6"},"source":["from tensorflow.keras.layers import Dropout"],"id":"398910c6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09b09062"},"source":[""],"id":"09b09062","execution_count":null,"outputs":[]}]}